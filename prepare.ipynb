{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ab4219-a6b6-47f3-b0a3-c3c865fe6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire as a\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4a44dc-d487-47e7-966d-6b18d1d6b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = a.acquire_edu_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f371dd-1e68-49d4-be82-511bdf70db37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First thing I want to do is standardize all the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c7fdea-b928-4ab6-a824-1f8c13198e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Gender', 'EthnicGroup', 'ParentEduc',\n",
       "       'LunchType', 'TestPrep', 'ParentMaritalStatus', 'PracticeSport',\n",
       "       'IsFirstChild', 'NrSiblings', 'TransportMeans', 'WklyStudyHours',\n",
       "       'MathScore', 'ReadingScore', 'WritingScore'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e20b2-f6d3-4f39-b567-1a4dbb1316ab",
   "metadata": {},
   "source": [
    "    Key takaways:\n",
    "    - there is a combination of upper and lowercase letters\n",
    "    - Some columns have sapces \n",
    "    - unnamed columns are mirror of index so we can remove those columns. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1349bc8-fa06-4d3e-b53c-5da8d566e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30641 entries, 0 to 30640\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0.1         30641 non-null  int64  \n",
      " 1   Unnamed: 0           30641 non-null  int64  \n",
      " 2   Gender               30641 non-null  object \n",
      " 3   EthnicGroup          28801 non-null  object \n",
      " 4   ParentEduc           28796 non-null  object \n",
      " 5   LunchType            30641 non-null  object \n",
      " 6   TestPrep             28811 non-null  object \n",
      " 7   ParentMaritalStatus  29451 non-null  object \n",
      " 8   PracticeSport        30010 non-null  object \n",
      " 9   IsFirstChild         29737 non-null  object \n",
      " 10  NrSiblings           29069 non-null  float64\n",
      " 11  TransportMeans       27507 non-null  object \n",
      " 12  WklyStudyHours       29686 non-null  object \n",
      " 13  MathScore            30641 non-null  int64  \n",
      " 14  ReadingScore         30641 non-null  int64  \n",
      " 15  WritingScore         30641 non-null  int64  \n",
      "dtypes: float64(1), int64(5), object(10)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# aets look at what our data is like \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a241ac9-a695-4d49-80f3-2f6de6807a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets addess the first two with one line of code\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7aa56d-029a-4cfb-ab96-cced432babcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for the unnamed columns\n",
    "df.drop(columns=[c for c in df.columns if 'unnamed' in c],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ca953-9558-47c0-8b51-35d7da38208d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Here we are looking to address the null values . This is a special case with nulls becasue every data point is a student. To use df.dropna() will drop a student who didnt have a voice and we don't want to take that away from a student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1caaca0-e7f4-478c-8330-85d7b88102ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                  0.000000\n",
       "ethnicgroup             6.005026\n",
       "parenteduc              6.021344\n",
       "lunchtype               0.000000\n",
       "testprep                5.972390\n",
       "parentmaritalstatus     3.883685\n",
       "practicesport           2.059332\n",
       "isfirstchild            2.950295\n",
       "nrsiblings              5.130381\n",
       "transportmeans         10.228126\n",
       "wklystudyhours          3.116739\n",
       "mathscore               0.000000\n",
       "readingscore            0.000000\n",
       "writingscore            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first I want to see how many missing values there are in the dataset and set it to output it by % of columns\n",
    "(df.isna().sum() / len(df) )* 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66590229-9d86-48eb-a1b9-687b4718c97f",
   "metadata": {},
   "source": [
    "    Addressing the first null, I'm gonna drop the column ethnicgroup. I did this to prevent any potential biases or unfair labeling based on ethnicity, and to ensure that the analysis is focused solely on the other factors that may be affecting educational performance. It is important to note that removing a variable like ethnicity from the analysis does not mean that it is not an important factor, but rather that in this specific analysis, we are choosing to focus on other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12aaee0d-7ddb-401a-8f3f-be27fb6138cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='ethnicgroup', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3b5fab-7c2f-4894-a85a-ba18e6d3a7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                  0.000000\n",
       "parenteduc              6.021344\n",
       "lunchtype               0.000000\n",
       "testprep                5.972390\n",
       "parentmaritalstatus     3.883685\n",
       "practicesport           2.059332\n",
       "isfirstchild            2.950295\n",
       "nrsiblings              5.130381\n",
       "transportmeans         10.228126\n",
       "wklystudyhours          3.116739\n",
       "mathscore               0.000000\n",
       "readingscore            0.000000\n",
       "writingscore            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first I want to see how many missing values there are in the dataset and set it to output it by % of columns\n",
    "(df.isna().sum() / len(df) )* 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689364c5-1ccc-49df-9481-87b0fedc7506",
   "metadata": {},
   "source": [
    "    We are going to impute the nulls until they are able to be update. Since all the nulls are less than 11 % of the columns in the dataset I feel comfortable enough to impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f9fa99-18a6-450d-a76f-778f9477cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_null =[]\n",
    "\n",
    "for cols in df.columns:\n",
    "    if df[cols].isna().sum() > 0:\n",
    "        has_null.append(cols)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2663af44-f8d6-4f76-989a-215ca01f1dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parenteduc',\n",
       " 'testprep',\n",
       " 'parentmaritalstatus',\n",
       " 'practicesport',\n",
       " 'isfirstchild',\n",
       " 'nrsiblings',\n",
       " 'transportmeans',\n",
       " 'wklystudyhours']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we trust but verify our code, and ensure that the null columns were added to the list\n",
    "has_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a553be88-6f7a-4ca3-9fdf-c809725588a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m df[col]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m             df[col] \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mfit_transform(df[col]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3949\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3950\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4134\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4135\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4143\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4146\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4147\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4148\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4149\u001b[0m     ):\n\u001b[1;32m   4150\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:4871\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m   4870\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m-> 4871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/pandas/core/construction.py:569\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    567\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 569\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subarr \u001b[38;5;129;01mis\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m copy:\n\u001b[1;32m    572\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m subarr\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1181\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(value))  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;66;03m# Caller is responsible\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(value\u001b[38;5;241m.\u001b[39mndim)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fill the missing values in each column\n",
    "for col in df.columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = imputer.fit_transform(df[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f06de6-2c13-4455-948e-49aa48470209",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isna().sum() / len(df) )* 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be072a8d-abf0-4f08-ac75-4aa5e0c69396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.testprep.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94ad77-cad0-4bce-a290-0b5feb8764a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225745f-9589-4b46-9d94-0c2630e7e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fb1b9-0d1f-46e5-a3d3-f83ddc0eb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68caef90-2d2d-4464-84b4-fce69d91218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lunchtype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fd708-c3da-4b1f-af72-6e108c5f9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['is_male', 'parent_educ', 'free_reduced_lunch', 'test_prep_completed', 'parent_marital_status',\n",
    "       'practicesport', 'is_first_child', 'nrsiblings', 'rides_bus',\n",
    "       'wkly_study_hours', 'math_score', 'reading_score', 'writing_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb54f22-34fe-41f8-8a50-1c85247f4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_change = {\n",
    "    'female' : 0,\n",
    "    'male' : 1,\n",
    "    'no' : 0,\n",
    "    'yes': 1,\n",
    "    'school_bus': 1,\n",
    "    'private' : 0,\n",
    "    'none' : 0,\n",
    "    'completed' : 1,\n",
    "    'sometimes' : 1,\n",
    "    'regularly' : 1,\n",
    "    'never' : 0,\n",
    "    'free/reduced' : 1,\n",
    "    'standard' : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdbc70-d57d-448e-9229-7cb177343592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(to_replace=value_change,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438110f7-c80f-4bca-82cc-d81056ef2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to engineer the final score.\n",
    "scores = ['writing_score' ,'reading_score', 'math_score']\n",
    "df['final_score'] =round( df[scores].mean(axis=1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c009ad-dc62-415b-bf8b-d3af4f30d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a203af-5252-418f-86db-32da9c8dc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = list()\n",
    "for cols in df.columns:\n",
    "    if df[cols].dtype == 'O':\n",
    "        object_columns.append(cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9171d-7642-47f2-a228-29b625907c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=object_columns, drop_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc27dee-a12b-449d-ae16-1b9dcd8a84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ff44e-c2c7-4808-91b0-68f3f3a5829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e6677-7c81-41ac-ab13-4fdefd7e0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    '''\n",
    "    This function splits a dataframe into \n",
    "    train, validate, and test in order to explore the data and to create and validate models. \n",
    "    It takes in a dataframe and contains an integer for setting a seed for replication. \n",
    "    Test is 20% of the original dataset. The remaining 80% of the dataset is \n",
    "    divided between valiidate and train, with validate being .30*.80= 24% of \n",
    "    the original dataset, and train being .70*.80= 56% of the original dataset. \n",
    "    The function returns, train, validate and test dataframes. \n",
    "    '''\n",
    "    train, test = train_test_split(df, test_size = .2, random_state=123)   \n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=123)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576eca49-91f3-4b3b-b64f-72f56cc9d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def prepare_edu():\n",
    "    df = a.acquire_edu_data()\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    df.drop(columns=[c for c in df.columns if 'unnamed' in c],inplace=True)\n",
    "    df.drop(columns='ethnicgroup', inplace=True)\n",
    "    df.drop(columns=[c for c in df.columns if 'unnamed' in c],inplace=True)\n",
    "    has_null =[]\n",
    "\n",
    "    for cols in df.columns:\n",
    "        if df[cols].isna().sum() > 0:\n",
    "            has_null.append(cols)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    # Fill the missing values in each column\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = imputer.fit_transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    df.columns = ['is_male', 'parent_educ', 'free_reduced_lunch', 'test_prep_completed', 'parent_marital_status',\n",
    "           'practicesport', 'is_first_child', 'nrsiblings', 'rides_bus',\n",
    "           'wkly_study_hours', 'math_score', 'reading_score', 'writing_score']\n",
    "\n",
    "    value_change = {\n",
    "    'female' : 0,\n",
    "    'male' : 1,\n",
    "    'no' : 0,\n",
    "    'yes': 1,\n",
    "    'school_bus': 1,\n",
    "    'private' : 0,\n",
    "    'none' : 0,\n",
    "    'completed' : 1,\n",
    "    'sometimes' : 1,\n",
    "    'regularly' : 1,\n",
    "    'never' : 0,\n",
    "    'free/reduced' : 1,\n",
    "    'standard' : 0\n",
    "    }\n",
    "    \n",
    "\n",
    "    df.replace(to_replace=value_change,inplace=True)\n",
    "\n",
    "    scores = ['writing_score' ,'reading_score', 'math_score']\n",
    "    df['final_score'] =round( df[scores].mean(axis=1),2)\n",
    "\n",
    "\n",
    "    object_columns = list()\n",
    "    for cols in df.columns:\n",
    "        if df[cols].dtype == 'O':\n",
    "            object_columns.append(cols)\n",
    "    df = pd.get_dummies(df, columns=object_columns, drop_first= True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7d10e-a7ab-4750-9fa7-aa8ca9d59632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  prepare_edu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4be1c-b2c2-4a87-8e34-10b259da71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a17fa3-f078-4327-bf3b-842979deb7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
